{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab14_2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akaver/NLP2019/blob/master/Lab14_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gt4kupNQ4u",
        "colab_type": "text"
      },
      "source": [
        "# Improving training\n",
        "\n",
        "In this lab, we will learn a few techniques to improve training neural nets. Most of the methods are not specific to speech and language, but can be used for any task where DNNs are used.\n",
        "\n",
        "The methods that will be covered are:\n",
        " \n",
        "\n",
        "*   Residual connections in DNNs\n",
        "*  Label smoothing\n",
        "*   Learning rate scheduling\n",
        "\n",
        "\n",
        "We will use the accent identification task as an example. \n",
        "\n",
        "\n",
        "We will use Pytorch to classify English speech segments according the native accent.\n",
        "\n",
        "Let's first download the (toy) data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_qGi_buNfKd",
        "colab_type": "code",
        "outputId": "b8b42083-e17f-4f0a-f44e-a4c5acadee68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "!rm -f accent_subset_wav.zip\n",
        "!wget https://phon.ioc.ee/~tanela/accent_subset_wav.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-30 10:06:32--  https://phon.ioc.ee/~tanela/accent_subset_wav.zip\n",
            "Resolving phon.ioc.ee (phon.ioc.ee)... 193.40.251.126\n",
            "Connecting to phon.ioc.ee (phon.ioc.ee)|193.40.251.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 503722423 (480M) [application/zip]\n",
            "Saving to: ‘accent_subset_wav.zip’\n",
            "\n",
            "accent_subset_wav.z 100%[===================>] 480.39M  19.8MB/s    in 41s     \n",
            "\n",
            "2019-04-30 10:07:14 (11.7 MB/s) - ‘accent_subset_wav.zip’ saved [503722423/503722423]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWNd9wYctWKW",
        "colab_type": "text"
      },
      "source": [
        "The data consists of wav files, and the filename says what accent the person had. The data originates from the CMU Speech Accent Database (http://accent.gmu.edu/).\n",
        "\n",
        "All speakers read the same passage: \"Please call Stella. Ask her to bring these things with her from the store: Six spoons of fresh snow peas, five thick slabs of blue cheese, and maybe a snack for her brother Bob. We also need a small plastic snake and a big toy frog for the kids. She can scoop these things into three red bags, and we will go meet her Wednesday at the train station.\"\n",
        "\n",
        "There are even some Estonian speakers in the database, you can listen to their speech here: http://accent.gmu.edu/browse_language.php?function=find&language=estonian. But in our experiments, we use only three accent groups: English (i.e., no accent), Arabic and Mandarin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9z1AI7RtcGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uack-fC-OtlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -rf data\n",
        "! mkdir -p data\n",
        "! unzip -q accent_subset_wav.zip -d data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGVZwha9tdj_",
        "colab_type": "code",
        "outputId": "855717fe-801e-482b-884f-2b463b4d6dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "! ls data/ | head"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "arabic100.wav\n",
            "arabic101.wav\n",
            "arabic102.wav\n",
            "arabic10.wav\n",
            "arabic11.wav\n",
            "arabic12.wav\n",
            "arabic13.wav\n",
            "arabic14.wav\n",
            "arabic15.wav\n",
            "arabic16.wav\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4wqLXUxti_N",
        "colab_type": "text"
      },
      "source": [
        "The dataset is highly skewed: there are much more native English speakers than Arabic and Mandarin speakers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xlJmpvvtk1k",
        "colab_type": "code",
        "outputId": "754b8b6d-e05a-4863-c987-a1dcf7708fdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "! ls data/english*.wav | wc -l\n",
        "! ls data/arabic*.wav | wc -l\n",
        "! ls data/mandarin*.wav | wc -l"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "579\n",
            "102\n",
            "65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FanVBeKvtwSV",
        "colab_type": "text"
      },
      "source": [
        "First, we will learn how to extract features from the audio. We will use the python_speech_features package for that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_24ktMBPLCt",
        "colab_type": "code",
        "outputId": "daf3c1fb-dba1-4c32-a7a8-dddb421eef61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "! pip install python_speech_features"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python_speech_features\n",
            "  Downloading https://files.pythonhosted.org/packages/ff/d1/94c59e20a2631985fbd2124c45177abaa9e0a4eee8ba8a305aa26fc02a8e/python_speech_features-0.6.tar.gz\n",
            "Building wheels for collected packages: python-speech-features\n",
            "  Building wheel for python-speech-features (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3c/42/7c/f60e9d1b40015cd69b213ad90f7c18a9264cd745b9888134be\n",
            "Successfully built python-speech-features\n",
            "Installing collected packages: python-speech-features\n",
            "Successfully installed python-speech-features-0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQp73QyDPRjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from python_speech_features import logfbank, mfcc\n",
        "import scipy.io.wavfile as wav\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FycoVANouAps",
        "colab_type": "text"
      },
      "source": [
        "I randomly split the dataset into train and dev set. Let's download the split definitions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqiglFOaPUR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! rm -f accent_trainset.txt accent_devset.txt\n",
        "! wget -q  https://phon.ioc.ee/~tanela/tmp/accent_trainset.txt https://phon.ioc.ee/~tanela/tmp/accent_devset.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26ukiHMSuKrt",
        "colab_type": "text"
      },
      "source": [
        "The files just list all the IDs in the train and dev set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7oprM1BuNrg",
        "colab_type": "code",
        "outputId": "57eb7e85-999e-458c-cf9e-1f1293fa12c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "! head accent_devset.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "english101\n",
            "mandarin11\n",
            "english229\n",
            "arabic7\n",
            "english466\n",
            "english516\n",
            "mandarin55\n",
            "english42\n",
            "english349\n",
            "english367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdaJiSw1PbFP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfGpzCP-PmLo",
        "colab_type": "code",
        "outputId": "9d52b772-e9e1-4de5-d0ac-ba302500ee72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFN1Qz6suS8C",
        "colab_type": "text"
      },
      "source": [
        "Let's define a class that can read and represent a dataset.\n",
        "\n",
        "In this lab, we will oonly use the 1st 10 seconds of every file, because handling audio files of different lenght is a bit complicated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6neln-naPoYW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AccentDataset(Dataset):\n",
        "    def __init__(self, ids_file, root_dir):\n",
        "               \n",
        "        self.data = []\n",
        "        ids = [l.strip() for l in open(ids_file).readlines()]\n",
        "        for i, id in enumerate(ids):\n",
        "          print(\"\\rReading %d-th file\" % i, end=\"\")\n",
        "          language_id = 0  \n",
        "          if id.startswith(\"english\"):\n",
        "            language_id = 1\n",
        "          elif id.startswith(\"mandarin\"):\n",
        "            language_id = 2\n",
        "          (rate,audio) = wav.read(\"data/%s.wav\" % id)\n",
        "          assert rate == 16000\n",
        "          # we use only the first 1000 feature vectors, i-e., the 1st 10 seconds of the audio         \n",
        "          features = logfbank(audio, 16000)[0:1000]\n",
        "          feat_tensor = torch.FloatTensor(features)\n",
        "          # each member in the dataset is a tuple of (features, language_id)\n",
        "          self.data.append((feat_tensor, language_id))\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpncGsoEPq7K",
        "colab_type": "code",
        "outputId": "ded4adf9-e864-4952-a25a-1032819c5b6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_dataset = AccentDataset(\"accent_trainset.txt\", root_dir=\"data\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading 675-th file"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gsV8IJUNPs_8",
        "colab_type": "code",
        "outputId": "19103bff-c9d4-45c0-b5d5-dca8b6708135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dev_dataset = AccentDataset(\"accent_devset.txt\", root_dir=\"data\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading 69-th file"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZmUrfkSP4n7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_iter = DataLoader(train_dataset, batch_size=32,  shuffle=True)\n",
        "dev_iter = DataLoader(dev_dataset, batch_size=32,  shuffle=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xl2GtQ6suzZG",
        "colab_type": "text"
      },
      "source": [
        "Now we can define our model.\n",
        "\n",
        "Note how the CNN model is very similar to the model we used for text classification. Only here we don't have to use word embeddings, because the speech features already look a bit like embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrQYQp2LP7Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import torch.nn.functional as F\n",
        "import sys"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VKu-6kpvP9hD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AccentCnn(nn.Module):\n",
        "  \n",
        "  def __init__(self, num_classes, feature_dim, dropout_prob=0.2):\n",
        "    super(AccentCnn, self).__init__()\n",
        "    self.input_bn = nn.BatchNorm1d(feature_dim)\n",
        "    self.conv1 = nn.Conv1d(feature_dim, 32, kernel_size=5, stride=1)\n",
        "    self.conv2 = nn.Conv1d(32, 64, kernel_size=5, stride=1)\n",
        "    self.conv3 = nn.Conv1d(64, 64, kernel_size=3, stride=1)\n",
        "    self.conv4 = nn.Conv1d(64, 64, kernel_size=3, stride=1)\n",
        "    \n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.fc = nn.Linear(64, num_classes)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Conv1d takes in (batch, channels, seq_len), but raw signal is (batch, seq_len, channels)\n",
        "    x = x.permute(0, 2, 1).contiguous()\n",
        "    x = self.input_bn(x)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = F.max_pool1d(x, 2)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = F.max_pool1d(x, 2)\n",
        "    #print(x.shape)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = F.max_pool1d(x, 2)\n",
        "    #print(x.shape)\n",
        "    \n",
        "    x = F.relu(self.conv4(x))\n",
        "    x = F.max_pool1d(x, x.size(2))\n",
        "    #print(x.shape)\n",
        "    x = x.view(-1, 64)\n",
        "    #print(x.shape)\n",
        "    x = self.dropout(x) \n",
        "    logit = self.fc(x)\n",
        "    return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR1DHLmfQAH-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cl06wRpQD3Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, num_epochs, train_iter, tdev_iter, device, log_interval=10):\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  steps = 0\n",
        "  best_acc = 0\n",
        "  last_step = 0\n",
        "  \n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(\"Starting epoch %d\" % epoch)\n",
        "    for batch in train_iter:\n",
        "      # set training mode\n",
        "      model.train()\n",
        "      fbank, target = batch\n",
        "      fbank, target = fbank.to(device), target.to(device)\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logit = model(fbank)\n",
        "\n",
        "      loss = F.cross_entropy(logit, target)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_acc = evaluate(\"train\", train_iter, model)                \n",
        "    dev_acc = evaluate(\"dev\", dev_iter, model)                \n",
        "\n",
        "def evaluate(name, data_iter, model):\n",
        "  # set evaluation mode (turns off dropout)\n",
        "  model.eval()\n",
        "  corrects, avg_loss = 0, 0\n",
        "  for batch in data_iter:\n",
        "    fbank, target = batch\n",
        "    fbank, target = fbank.to(device), target.to(device)\n",
        "    \n",
        "    logit = model(fbank)\n",
        "    loss = F.cross_entropy(logit, target,  reduction='sum')\n",
        "\n",
        "    avg_loss += loss.item()\n",
        "    corrects += (torch.max(logit, 1)[1].view(target.size()).data == target.data).sum()\n",
        "\n",
        "  size = len(data_iter.dataset)\n",
        "  avg_loss /= size\n",
        "  accuracy = 100.0 * float(corrects)/size\n",
        "  print('  Evaluation on {} - loss: {:.6f}  acc: {:.4f}%({}/{})'.format(name,\n",
        "                                                                        avg_loss, \n",
        "                                                                        accuracy, \n",
        "                                                                        corrects, \n",
        "                                                                        size))\n",
        "  return accuracy, avg_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7zVG7goQI0R",
        "colab_type": "code",
        "outputId": "de12d36f-7c56-4bd1-aa1a-fb3601c0ae74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1097
        }
      },
      "source": [
        "model = AccentCnn(3, train_dataset[0][0].shape[1]).to(device)\n",
        "train(model, 20, train_iter, dev_iter, device=device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Evaluation on train - loss: 0.723732  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.587152  acc: 81.4286%(57/70)\n",
            "Starting epoch 2\n",
            "Evaluation on train - loss: 0.718906  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.595017  acc: 81.4286%(57/70)\n",
            "Starting epoch 3\n",
            "Evaluation on train - loss: 0.668370  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.567475  acc: 81.4286%(57/70)\n",
            "Starting epoch 4\n",
            "Evaluation on train - loss: 0.654811  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.568043  acc: 81.4286%(57/70)\n",
            "Starting epoch 5\n",
            "Evaluation on train - loss: 0.655934  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.579484  acc: 81.4286%(57/70)\n",
            "Starting epoch 6\n",
            "Evaluation on train - loss: 0.638122  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.563410  acc: 81.4286%(57/70)\n",
            "Starting epoch 7\n",
            "Evaluation on train - loss: 0.626791  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.559641  acc: 81.4286%(57/70)\n",
            "Starting epoch 8\n",
            "Evaluation on train - loss: 0.614721  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.565457  acc: 81.4286%(57/70)\n",
            "Starting epoch 9\n",
            "Evaluation on train - loss: 0.633511  acc: 77.5148%(524/676)\n",
            "Evaluation on dev - loss: 0.560188  acc: 81.4286%(57/70)\n",
            "Starting epoch 10\n",
            "Evaluation on train - loss: 0.622374  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.548828  acc: 81.4286%(57/70)\n",
            "Starting epoch 11\n",
            "Evaluation on train - loss: 0.594207  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.513576  acc: 81.4286%(57/70)\n",
            "Starting epoch 12\n",
            "Evaluation on train - loss: 0.573822  acc: 79.1420%(535/676)\n",
            "Evaluation on dev - loss: 0.486622  acc: 81.4286%(57/70)\n",
            "Starting epoch 13\n",
            "Evaluation on train - loss: 0.567563  acc: 77.9586%(527/676)\n",
            "Evaluation on dev - loss: 0.496582  acc: 81.4286%(57/70)\n",
            "Starting epoch 14\n",
            "Evaluation on train - loss: 0.515555  acc: 80.1775%(542/676)\n",
            "Evaluation on dev - loss: 0.431274  acc: 84.2857%(59/70)\n",
            "Starting epoch 15\n",
            "Evaluation on train - loss: 0.488740  acc: 80.6213%(545/676)\n",
            "Evaluation on dev - loss: 0.425179  acc: 82.8571%(58/70)\n",
            "Starting epoch 16\n",
            "Evaluation on train - loss: 0.508990  acc: 79.2899%(536/676)\n",
            "Evaluation on dev - loss: 0.451129  acc: 82.8571%(58/70)\n",
            "Starting epoch 17\n",
            "Evaluation on train - loss: 0.481743  acc: 83.1361%(562/676)\n",
            "Evaluation on dev - loss: 0.429312  acc: 82.8571%(58/70)\n",
            "Starting epoch 18\n",
            "Evaluation on train - loss: 0.432177  acc: 84.0237%(568/676)\n",
            "Evaluation on dev - loss: 0.423740  acc: 82.8571%(58/70)\n",
            "Starting epoch 19\n",
            "Evaluation on train - loss: 0.401817  acc: 85.5030%(578/676)\n",
            "Evaluation on dev - loss: 0.425344  acc: 81.4286%(57/70)\n",
            "Starting epoch 20\n",
            "Evaluation on train - loss: 0.393525  acc: 85.0592%(575/676)\n",
            "Evaluation on dev - loss: 0.382479  acc: 84.2857%(59/70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PtEuwGs5v8bP",
        "colab_type": "text"
      },
      "source": [
        "Note how the model reached accuracy of 81% already after the 1st epoch. But let's not fool ourselves. The dataset is highly skewed, and therefore 57 speakers in the dev data are English natives. So, the model quickly learned that it's a good idea to classify everybody as English, and this gives us already 81%. Of course, we don't need a complex neural network model to do that. We can see that the final model classifies only a few other items besides the 57 English ones correctly. This is not really surprising, because the dataset is rather small and classifying speech is more difficult than classifying text, where certian words can act as very good hints for classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSXl4x54v7cG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYk-l9ewQREF",
        "colab_type": "text"
      },
      "source": [
        "## Residual connections\n",
        "\n",
        "(Based on https://towardsdatascience.com/an-overview-of-resnet-and-its-variants-5281e2f56035)\n",
        "\n",
        "According to the universal approximation theorem, given enough capacity, we know that a feedforward network with a single layer is sufficient to represent any function. However, the layer might be massive and the network is prone to overfitting the data. Therefore, there is a common trend in the research community that our network architecture needs to go deeper.\n",
        "\n",
        "However, increasing network depth does not work by simply stacking layers together. Deep networks are hard to train because of the notorious vanishing gradient problem — as the gradient is back-propagated to earlier layers, repeated multiplication may make the gradient infinitively small. As a result, as the network goes deeper, its performance gets saturated or even starts degrading rapidly.\n",
        "\n",
        "The core idea of ResNet is introducing a so-called “identity shortcut connection” that skips one or more layers, as shown in the following figure:\n",
        "\n",
        "![Resnet](https://cdn-images-1.medium.com/max/1200/1*ByrVJspW-TefwlH7OLxNkg.png)\n",
        "\n",
        "The authors argue that stacking layers shouldn’t degrade the network performance, because we could simply stack identity mappings (layer that doesn’t do anything) upon the current network, and the resulting architecture would perform the same. This indicates that the deeper model should not produce a training error higher than its shallower counterparts. They hypothesize that letting the stacked layers fit a residual mapping is easier than letting them directly fit the desired underlaying mapping. And the residual block above explicitly allows it to do precisely that.\n",
        "\n",
        "Nowadays, there are many variant of ResNets (see below), we will only implement the original one.\n",
        "\n",
        "![Resnet variants](https://cdn-images-1.medium.com/max/1600/1*M5NIelQC33eN6KjwZRccoQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBdOQ93iQLs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv1d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm1d(dim)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv1d(dim, dim, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm1d(dim)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        # add the original input and the transformed input together\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vGSZIXkRRZn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class AccentResNet(nn.Module):\n",
        "  \n",
        "  def __init__(self, num_classes, feature_dim, dropout_prob=0.2):\n",
        "    super(AccentResNet, self).__init__()\n",
        "    self.layers = nn.Sequential(\n",
        "        # First, apply a simple Convnet to raw input\n",
        "        nn.BatchNorm1d(feature_dim),\n",
        "        nn.Conv1d(feature_dim, 32, kernel_size=5, stride=1),\n",
        "        nn.ReLU(inplace=True),\n",
        "        \n",
        "        # Now, apply the first residual block\n",
        "        ResidualBlock(32),\n",
        "        \n",
        "        # Just increase the number of filters using a a convolution with kernel size 1\n",
        "        nn.Conv1d(32, 48, kernel_size=1, stride=1),\n",
        "        # And compress the signmal in time\n",
        "        nn.MaxPool1d(kernel_size=2),\n",
        "        \n",
        "        # Now, apply the second residual block\n",
        "        ResidualBlock(48),\n",
        "\n",
        "        nn.Conv1d(48, 64, kernel_size=1, stride=1),\n",
        "        nn.MaxPool1d(kernel_size=2),\n",
        "        \n",
        "        # Now, apply the third residual block\n",
        "        ResidualBlock(64),        \n",
        "    )\n",
        "    \n",
        "    self.dropout = nn.Dropout(dropout_prob)\n",
        "    self.fc = nn.Linear(64, num_classes)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    # Conv1d takes in (batch, channels, seq_len), but raw signal is (batch, seq_len, channels)\n",
        "    x = x.permute(0, 2, 1).contiguous()\n",
        "    x = self.layers(x)\n",
        "  \n",
        "    \n",
        "    # global max\n",
        "    x = F.avg_pool1d(x, x.size(2))\n",
        "    #print(x.shape)\n",
        "    x = x.view(-1, 64)\n",
        "    #print(x.shape)\n",
        "    x = self.dropout(x) \n",
        "    logit = self.fc(x)\n",
        "    return logit"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q8ENS1ZbFhr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_resnet = AccentResNet(3, train_dataset[0][0].shape[1]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bAP0juO7bRX5",
        "colab_type": "code",
        "outputId": "6f04afba-ac06-483a-e796-c248810a7e6d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        }
      },
      "source": [
        "model_resnet.forward(next(iter(dev_iter))[0].to(device))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3262,  0.2829, -0.1772],\n",
              "        [ 0.0474,  0.1098, -0.1222],\n",
              "        [ 0.0927,  0.1707, -0.0980],\n",
              "        [ 0.2885,  0.1500, -0.0885],\n",
              "        [ 0.0090,  0.2199, -0.0631],\n",
              "        [ 0.1040,  0.3576, -0.0077],\n",
              "        [ 0.1578,  0.2775, -0.0416],\n",
              "        [ 0.1955,  0.1735, -0.1354],\n",
              "        [ 0.0630,  0.1884, -0.0043],\n",
              "        [ 0.1822,  0.3294, -0.1233],\n",
              "        [ 0.1563,  0.2607, -0.1367],\n",
              "        [ 0.0560,  0.3536, -0.1032],\n",
              "        [ 0.1126,  0.1777,  0.0370],\n",
              "        [ 0.0725,  0.0604,  0.0303],\n",
              "        [ 0.0626,  0.3047, -0.1097],\n",
              "        [ 0.1962,  0.1891,  0.0445],\n",
              "        [ 0.2630,  0.2101, -0.1371],\n",
              "        [ 0.1929,  0.3090, -0.2444],\n",
              "        [ 0.1634,  0.0663,  0.0884],\n",
              "        [ 0.2306,  0.1914, -0.0806],\n",
              "        [ 0.2364,  0.3678, -0.1532],\n",
              "        [ 0.0883,  0.4016, -0.1731],\n",
              "        [ 0.1225, -0.0120,  0.0658],\n",
              "        [ 0.0733,  0.1131,  0.0288],\n",
              "        [ 0.2509,  0.2361,  0.0050],\n",
              "        [ 0.1716,  0.2496, -0.2303],\n",
              "        [ 0.1247,  0.2450, -0.2005],\n",
              "        [ 0.1595,  0.2046,  0.1243],\n",
              "        [ 0.2985,  0.0983,  0.1967],\n",
              "        [ 0.1707,  0.0726,  0.1644],\n",
              "        [ 0.1634,  0.1977, -0.1054],\n",
              "        [ 0.1600,  0.0128, -0.0185]], device='cuda:0', grad_fn=<AddmmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YS8AnbWobyvp",
        "colab_type": "code",
        "outputId": "c1d01ccf-0605-45b2-c8d1-5caaee585fe4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1097
        }
      },
      "source": [
        "model_resnet = AccentResNet(3, train_dataset[0][0].shape[1]).to(device)\n",
        "train(model_resnet, 20, train_iter, dev_iter, device=device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n",
            "Evaluation on train - loss: 0.722397  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.658993  acc: 81.4286%(57/70)\n",
            "Starting epoch 2\n",
            "Evaluation on train - loss: 0.647977  acc: 77.8107%(526/676)\n",
            "Evaluation on dev - loss: 0.463318  acc: 80.0000%(56/70)\n",
            "Starting epoch 3\n",
            "Evaluation on train - loss: 0.607394  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.486558  acc: 81.4286%(57/70)\n",
            "Starting epoch 4\n",
            "Evaluation on train - loss: 0.558641  acc: 78.5503%(531/676)\n",
            "Evaluation on dev - loss: 0.428126  acc: 81.4286%(57/70)\n",
            "Starting epoch 5\n",
            "Evaluation on train - loss: 0.574249  acc: 78.2544%(529/676)\n",
            "Evaluation on dev - loss: 0.449939  acc: 81.4286%(57/70)\n",
            "Starting epoch 6\n",
            "Evaluation on train - loss: 0.534179  acc: 79.7337%(539/676)\n",
            "Evaluation on dev - loss: 0.442328  acc: 85.7143%(60/70)\n",
            "Starting epoch 7\n",
            "Evaluation on train - loss: 0.508158  acc: 82.1006%(555/676)\n",
            "Evaluation on dev - loss: 0.332880  acc: 88.5714%(62/70)\n",
            "Starting epoch 8\n",
            "Evaluation on train - loss: 0.508253  acc: 82.2485%(556/676)\n",
            "Evaluation on dev - loss: 0.401687  acc: 85.7143%(60/70)\n",
            "Starting epoch 9\n",
            "Evaluation on train - loss: 0.471205  acc: 80.6213%(545/676)\n",
            "Evaluation on dev - loss: 0.394629  acc: 84.2857%(59/70)\n",
            "Starting epoch 10\n",
            "Evaluation on train - loss: 0.429809  acc: 83.7278%(566/676)\n",
            "Evaluation on dev - loss: 0.294165  acc: 88.5714%(62/70)\n",
            "Starting epoch 11\n",
            "Evaluation on train - loss: 0.411846  acc: 84.0237%(568/676)\n",
            "Evaluation on dev - loss: 0.259991  acc: 88.5714%(62/70)\n",
            "Starting epoch 12\n",
            "Evaluation on train - loss: 0.547212  acc: 78.9941%(534/676)\n",
            "Evaluation on dev - loss: 0.500583  acc: 82.8571%(58/70)\n",
            "Starting epoch 13\n",
            "Evaluation on train - loss: 0.391957  acc: 85.6509%(579/676)\n",
            "Evaluation on dev - loss: 0.289165  acc: 88.5714%(62/70)\n",
            "Starting epoch 14\n",
            "Evaluation on train - loss: 0.381371  acc: 86.0947%(582/676)\n",
            "Evaluation on dev - loss: 0.318603  acc: 90.0000%(63/70)\n",
            "Starting epoch 15\n",
            "Evaluation on train - loss: 0.455291  acc: 79.5858%(538/676)\n",
            "Evaluation on dev - loss: 0.425217  acc: 81.4286%(57/70)\n",
            "Starting epoch 16\n",
            "Evaluation on train - loss: 0.336671  acc: 86.2426%(583/676)\n",
            "Evaluation on dev - loss: 0.325946  acc: 90.0000%(63/70)\n",
            "Starting epoch 17\n",
            "Evaluation on train - loss: 0.512370  acc: 81.3609%(550/676)\n",
            "Evaluation on dev - loss: 0.360325  acc: 88.5714%(62/70)\n",
            "Starting epoch 18\n",
            "Evaluation on train - loss: 0.571110  acc: 77.6627%(525/676)\n",
            "Evaluation on dev - loss: 0.451409  acc: 85.7143%(60/70)\n",
            "Starting epoch 19\n",
            "Evaluation on train - loss: 0.330235  acc: 87.2781%(590/676)\n",
            "Evaluation on dev - loss: 0.353904  acc: 88.5714%(62/70)\n",
            "Starting epoch 20\n",
            "Evaluation on train - loss: 0.359360  acc: 84.3195%(570/676)\n",
            "Evaluation on dev - loss: 0.357706  acc: 87.1429%(61/70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYDFFpubfcLl",
        "colab_type": "text"
      },
      "source": [
        "## Label smoothing\n",
        "\n",
        "When we apply the cross-entropy loss to a classification task, we're expecting true labels to have 1, while the others 0. In other words, we have no doubts that the true labels are true, and the others are not. Is that always true? Maybe not. Many manual annotations are the results of multiple participants. They might have different criteria. They might make some mistakes. They are human, after all. As a result, the ground truth labels we have had perfect beliefs on are possible wrong.\n",
        "\n",
        "One possibile solution to this is to relax our confidence on the labels. For instance, we can slighly lower the loss target values from 1 to, say, 0.9. And naturally we increase the target value of 0 for the others slightly as such. This idea is called label smoothing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXXHiFdAb4Kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_label_smoothing(model, num_epochs, train_iter, test_iter, device,  label_smoothing=0.1, log_interval=10):\n",
        "\n",
        "  assert (label_smoothing >= 0.0 and label_smoothing <= 1.0)\n",
        "  \n",
        "  # Each non-target class gets a target probability of label_smoothing / 2.0\n",
        "  non_target_prob = label_smoothing / 2.0\n",
        "    \n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "  steps = 0\n",
        "  best_acc = 0\n",
        "  last_step = 0\n",
        "  \n",
        "  criterion = nn.KLDivLoss(reduction='sum')\n",
        "  \n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(\"Starting epoch %d\" % epoch)\n",
        "    for batch in train_iter:\n",
        "      # set training mode\n",
        "      model.train()\n",
        "      fbank, target = batch\n",
        "      fbank, target = fbank.to(device), target.to(device)\n",
        "      batch_size = target.shape[0]\n",
        "      \n",
        "      # Create a one-hot encoded target matrix of size (batch_size, 3)\n",
        "      # where the values corresponding to correct labelsa re 1.0-label_smoothing\n",
        "      # and everywhere else there is non_target_prob (equal to 0.05, if label_smoothing is 0.1)\n",
        "      target_one_hot = torch.ones(batch_size, 3).to(device)      \n",
        "      target_one_hot *= non_target_prob\n",
        "      target_one_hot[torch.arange(batch_size, dtype=torch.int64).to(device), target] = 1.0 - label_smoothing\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logit = model(fbank)\n",
        "      log_probabilities = F.log_softmax(logit)\n",
        "      loss = criterion(log_probabilities, target_one_hot).mean()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "\n",
        "    train_acc = evaluate(\"train\", train_iter, model)                \n",
        "    dev_acc = evaluate(\"dev\", dev_iter, model)                \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEssk12LjT28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPYwNZ6_jcSG",
        "colab_type": "code",
        "outputId": "f7646d54-3b29-4f0b-97cb-26fb13632364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1655
        }
      },
      "source": [
        "model_resnet = AccentResNet(3, train_dataset[0][0].shape[1]).to(device)\n",
        "train_with_label_smoothing(model_resnet, 30, train_iter, dev_iter, device=device, label_smoothing=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Evaluation on train - loss: 0.893660  acc: 77.2189%(522/676)\n",
            "Evaluation on dev - loss: 0.865731  acc: 81.4286%(57/70)\n",
            "Starting epoch 2\n",
            "Evaluation on train - loss: 0.770791  acc: 77.9586%(527/676)\n",
            "Evaluation on dev - loss: 0.710623  acc: 81.4286%(57/70)\n",
            "Starting epoch 3\n",
            "Evaluation on train - loss: 0.635451  acc: 79.4379%(537/676)\n",
            "Evaluation on dev - loss: 0.573594  acc: 82.8571%(58/70)\n",
            "Starting epoch 4\n",
            "Evaluation on train - loss: 0.610297  acc: 80.6213%(545/676)\n",
            "Evaluation on dev - loss: 0.581662  acc: 82.8571%(58/70)\n",
            "Starting epoch 5\n",
            "Evaluation on train - loss: 0.560857  acc: 81.8047%(553/676)\n",
            "Evaluation on dev - loss: 0.505400  acc: 85.7143%(60/70)\n",
            "Starting epoch 6\n",
            "Evaluation on train - loss: 0.566048  acc: 82.9882%(561/676)\n",
            "Evaluation on dev - loss: 0.502643  acc: 88.5714%(62/70)\n",
            "Starting epoch 7\n",
            "Evaluation on train - loss: 0.507788  acc: 80.6213%(545/676)\n",
            "Evaluation on dev - loss: 0.453049  acc: 84.2857%(59/70)\n",
            "Starting epoch 8\n",
            "Evaluation on train - loss: 0.590833  acc: 84.9112%(574/676)\n",
            "Evaluation on dev - loss: 0.583082  acc: 88.5714%(62/70)\n",
            "Starting epoch 9\n",
            "Evaluation on train - loss: 0.605602  acc: 85.6509%(579/676)\n",
            "Evaluation on dev - loss: 0.563043  acc: 88.5714%(62/70)\n",
            "Starting epoch 10\n",
            "Evaluation on train - loss: 0.483452  acc: 78.6982%(532/676)\n",
            "Evaluation on dev - loss: 0.514843  acc: 81.4286%(57/70)\n",
            "Starting epoch 11\n",
            "Evaluation on train - loss: 0.518209  acc: 86.6864%(586/676)\n",
            "Evaluation on dev - loss: 0.546133  acc: 85.7143%(60/70)\n",
            "Starting epoch 12\n",
            "Evaluation on train - loss: 0.541235  acc: 87.4260%(591/676)\n",
            "Evaluation on dev - loss: 0.610930  acc: 84.2857%(59/70)\n",
            "Starting epoch 13\n",
            "Evaluation on train - loss: 0.504035  acc: 88.3136%(597/676)\n",
            "Evaluation on dev - loss: 0.501487  acc: 90.0000%(63/70)\n",
            "Starting epoch 14\n",
            "Evaluation on train - loss: 0.454482  acc: 86.8343%(587/676)\n",
            "Evaluation on dev - loss: 0.513210  acc: 88.5714%(62/70)\n",
            "Starting epoch 15\n",
            "Evaluation on train - loss: 0.647850  acc: 86.8343%(587/676)\n",
            "Evaluation on dev - loss: 0.667874  acc: 84.2857%(59/70)\n",
            "Starting epoch 16\n",
            "Evaluation on train - loss: 0.446386  acc: 89.0533%(602/676)\n",
            "Evaluation on dev - loss: 0.414881  acc: 91.4286%(64/70)\n",
            "Starting epoch 17\n",
            "Evaluation on train - loss: 0.533492  acc: 90.5325%(612/676)\n",
            "Evaluation on dev - loss: 0.564313  acc: 85.7143%(60/70)\n",
            "Starting epoch 18\n",
            "Evaluation on train - loss: 0.644239  acc: 82.9882%(561/676)\n",
            "Evaluation on dev - loss: 0.702925  acc: 78.5714%(55/70)\n",
            "Starting epoch 19\n",
            "Evaluation on train - loss: 0.729472  acc: 78.9941%(534/676)\n",
            "Evaluation on dev - loss: 0.751871  acc: 77.1429%(54/70)\n",
            "Starting epoch 20\n",
            "Evaluation on train - loss: 0.441753  acc: 90.8284%(614/676)\n",
            "Evaluation on dev - loss: 0.536701  acc: 87.1429%(61/70)\n",
            "Starting epoch 21\n",
            "Evaluation on train - loss: 0.445335  acc: 92.8994%(628/676)\n",
            "Evaluation on dev - loss: 0.533764  acc: 85.7143%(60/70)\n",
            "Starting epoch 22\n",
            "Evaluation on train - loss: 0.400491  acc: 95.1183%(643/676)\n",
            "Evaluation on dev - loss: 0.499356  acc: 84.2857%(59/70)\n",
            "Starting epoch 23\n",
            "Evaluation on train - loss: 0.398687  acc: 92.6036%(626/676)\n",
            "Evaluation on dev - loss: 0.541447  acc: 84.2857%(59/70)\n",
            "Starting epoch 24\n",
            "Evaluation on train - loss: 0.361314  acc: 93.1953%(630/676)\n",
            "Evaluation on dev - loss: 0.433222  acc: 91.4286%(64/70)\n",
            "Starting epoch 25\n",
            "Evaluation on train - loss: 0.340792  acc: 87.4260%(591/676)\n",
            "Evaluation on dev - loss: 0.412897  acc: 82.8571%(58/70)\n",
            "Starting epoch 26\n",
            "Evaluation on train - loss: 0.332088  acc: 85.0592%(575/676)\n",
            "Evaluation on dev - loss: 0.466891  acc: 84.2857%(59/70)\n",
            "Starting epoch 27\n",
            "Evaluation on train - loss: 0.355641  acc: 96.5976%(653/676)\n",
            "Evaluation on dev - loss: 0.501535  acc: 85.7143%(60/70)\n",
            "Starting epoch 28\n",
            "Evaluation on train - loss: 0.656700  acc: 79.8817%(540/676)\n",
            "Evaluation on dev - loss: 0.741951  acc: 71.4286%(50/70)\n",
            "Starting epoch 29\n",
            "Evaluation on train - loss: 0.448829  acc: 95.5621%(646/676)\n",
            "Evaluation on dev - loss: 0.598547  acc: 82.8571%(58/70)\n",
            "Starting epoch 30\n",
            "Evaluation on train - loss: 0.340004  acc: 94.9704%(642/676)\n",
            "Evaluation on dev - loss: 0.422684  acc: 90.0000%(63/70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRWyau_4jsMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdZXQVMdp-zU",
        "colab_type": "text"
      },
      "source": [
        "## Learning rate scheduling\n",
        "\n",
        "(Based on https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1)\n",
        "\n",
        "When training deep neural networks, it is often useful to reduce learning rate as the training progresses. This can be done by using pre-defined learning rate schedules or adaptive learning rate methods.\n",
        "\n",
        "### Learning Rate Schedules\n",
        "Learning rate schedules seek to adjust the learning rate during training by reducing the learning rate according to a pre-defined schedule. Common learning rate schedules include time-based decay, step decay and exponential decay.\n",
        "\n",
        "Step decay schedule drops the learning rate by a factor every epoch, or evry few epochs. It's easy to implement using Pytorch's builtin class, as seen below.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJHPkn8FqBLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_label_smoothing_lr_schedule(model, num_epochs, train_iter, test_iter, device,  label_smoothing=0.1, lr_decay=0.5, log_interval=10):\n",
        "\n",
        "  assert (label_smoothing >= 0.0 and label_smoothing <= 1.0)\n",
        "  \n",
        "  # Each non-target class gets a target probability of label_smoothing / 2.0\n",
        "  non_target_prob = label_smoothing / 2.0\n",
        "  \n",
        "  # Usually, SGD is used instead of Adam, when learning rate scheduling is used\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "  \n",
        "  # every 2 epochs, we will multiply the learning rate by 0.5\n",
        "  lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=lr_decay)\n",
        "\n",
        "  steps = 0\n",
        "  best_acc = 0\n",
        "  last_step = 0\n",
        "  \n",
        "  criterion = nn.KLDivLoss(reduction='sum')\n",
        "  \n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    lr_scheduler.step()\n",
        "    print(\"Starting epoch %d, learning rate is %f\" % (epoch, lr_scheduler.get_lr()[0]))\n",
        "    for batch in train_iter:\n",
        "      # set training mode\n",
        "      model.train()\n",
        "      fbank, target = batch\n",
        "      fbank, target = fbank.to(device), target.to(device)\n",
        "      batch_size = target.shape[0]\n",
        "      \n",
        "      # Create a one-hot encoded target matrix of size (batch_size, 3)\n",
        "      # where the values corresponding to correct labelsa re 1.0-label_smoothing\n",
        "      # and everywhere else there is non_target_prob (equal to 0.05, if label_smoothing is 0.1)\n",
        "      target_one_hot = torch.ones(batch_size, 3).to(device)      \n",
        "      target_one_hot *= non_target_prob\n",
        "      target_one_hot[torch.arange(batch_size, dtype=torch.int64).to(device), target] = 1.0 - label_smoothing\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logit = model(fbank)\n",
        "      log_probabilities = F.log_softmax(logit)\n",
        "      loss = criterion(log_probabilities, target_one_hot).mean()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    train_acc = evaluate(\"train\", train_iter, model)                \n",
        "    dev_acc = evaluate(\"dev\", dev_iter, model)       "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkyKqi95sF9W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61s2IcfHsJk0",
        "colab_type": "code",
        "outputId": "02edf66c-2f1f-4651-dad0-7358cbbc88cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1655
        }
      },
      "source": [
        "model_resnet = AccentResNet(3, train_dataset[0][0].shape[1]).to(device)\n",
        "train_with_label_smoothing_lr_schedule(model_resnet, 30, train_iter, dev_iter, device=device, label_smoothing=0.2, lr_decay=0.9)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1, learning rate is 0.010000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Evaluation on train - loss: 0.790495  acc: 77.2189%(522/676)\n",
            "  Evaluation on dev - loss: 0.734411  acc: 81.4286%(57/70)\n",
            "Starting epoch 2, learning rate is 0.009000\n",
            "  Evaluation on train - loss: 0.762367  acc: 78.4024%(530/676)\n",
            "  Evaluation on dev - loss: 0.723188  acc: 82.8571%(58/70)\n",
            "Starting epoch 3, learning rate is 0.008100\n",
            "  Evaluation on train - loss: 1.033133  acc: 47.9290%(324/676)\n",
            "  Evaluation on dev - loss: 0.905768  acc: 60.0000%(42/70)\n",
            "Starting epoch 4, learning rate is 0.007290\n",
            "  Evaluation on train - loss: 0.673452  acc: 78.9941%(534/676)\n",
            "  Evaluation on dev - loss: 0.623219  acc: 78.5714%(55/70)\n",
            "Starting epoch 5, learning rate is 0.006561\n",
            "  Evaluation on train - loss: 0.591710  acc: 78.8462%(533/676)\n",
            "  Evaluation on dev - loss: 0.505518  acc: 80.0000%(56/70)\n",
            "Starting epoch 6, learning rate is 0.005905\n",
            "  Evaluation on train - loss: 0.745496  acc: 78.2544%(529/676)\n",
            "  Evaluation on dev - loss: 0.674897  acc: 84.2857%(59/70)\n",
            "Starting epoch 7, learning rate is 0.005314\n",
            "  Evaluation on train - loss: 0.548542  acc: 77.9586%(527/676)\n",
            "  Evaluation on dev - loss: 0.479472  acc: 81.4286%(57/70)\n",
            "Starting epoch 8, learning rate is 0.004783\n",
            "  Evaluation on train - loss: 0.523630  acc: 81.2130%(549/676)\n",
            "  Evaluation on dev - loss: 0.450045  acc: 84.2857%(59/70)\n",
            "Starting epoch 9, learning rate is 0.004305\n",
            "  Evaluation on train - loss: 0.679560  acc: 80.6213%(545/676)\n",
            "  Evaluation on dev - loss: 0.594435  acc: 87.1429%(61/70)\n",
            "Starting epoch 10, learning rate is 0.003874\n",
            "  Evaluation on train - loss: 0.525022  acc: 81.5089%(551/676)\n",
            "  Evaluation on dev - loss: 0.497989  acc: 84.2857%(59/70)\n",
            "Starting epoch 11, learning rate is 0.003487\n",
            "  Evaluation on train - loss: 0.507536  acc: 81.2130%(549/676)\n",
            "  Evaluation on dev - loss: 0.491598  acc: 82.8571%(58/70)\n",
            "Starting epoch 12, learning rate is 0.003138\n",
            "  Evaluation on train - loss: 0.508200  acc: 84.3195%(570/676)\n",
            "  Evaluation on dev - loss: 0.485819  acc: 88.5714%(62/70)\n",
            "Starting epoch 13, learning rate is 0.002824\n",
            "  Evaluation on train - loss: 0.499185  acc: 84.4675%(571/676)\n",
            "  Evaluation on dev - loss: 0.459613  acc: 87.1429%(61/70)\n",
            "Starting epoch 14, learning rate is 0.002542\n",
            "  Evaluation on train - loss: 0.469744  acc: 83.2840%(563/676)\n",
            "  Evaluation on dev - loss: 0.419229  acc: 87.1429%(61/70)\n",
            "Starting epoch 15, learning rate is 0.002288\n",
            "  Evaluation on train - loss: 0.462013  acc: 84.4675%(571/676)\n",
            "  Evaluation on dev - loss: 0.457811  acc: 85.7143%(60/70)\n",
            "Starting epoch 16, learning rate is 0.002059\n",
            "  Evaluation on train - loss: 0.473262  acc: 85.5030%(578/676)\n",
            "  Evaluation on dev - loss: 0.453106  acc: 90.0000%(63/70)\n",
            "Starting epoch 17, learning rate is 0.001853\n",
            "  Evaluation on train - loss: 0.434309  acc: 86.0947%(582/676)\n",
            "  Evaluation on dev - loss: 0.423193  acc: 90.0000%(63/70)\n",
            "Starting epoch 18, learning rate is 0.001668\n",
            "  Evaluation on train - loss: 0.483388  acc: 87.7219%(593/676)\n",
            "  Evaluation on dev - loss: 0.493680  acc: 91.4286%(64/70)\n",
            "Starting epoch 19, learning rate is 0.001501\n",
            "  Evaluation on train - loss: 0.470122  acc: 86.5385%(585/676)\n",
            "  Evaluation on dev - loss: 0.494036  acc: 88.5714%(62/70)\n",
            "Starting epoch 20, learning rate is 0.001351\n",
            "  Evaluation on train - loss: 0.391872  acc: 85.7988%(580/676)\n",
            "  Evaluation on dev - loss: 0.383359  acc: 88.5714%(62/70)\n",
            "Starting epoch 21, learning rate is 0.001216\n",
            "  Evaluation on train - loss: 0.413154  acc: 88.0178%(595/676)\n",
            "  Evaluation on dev - loss: 0.431701  acc: 90.0000%(63/70)\n",
            "Starting epoch 22, learning rate is 0.001094\n",
            "  Evaluation on train - loss: 0.449144  acc: 89.0533%(602/676)\n",
            "  Evaluation on dev - loss: 0.452085  acc: 90.0000%(63/70)\n",
            "Starting epoch 23, learning rate is 0.000985\n",
            "  Evaluation on train - loss: 0.488503  acc: 89.7929%(607/676)\n",
            "  Evaluation on dev - loss: 0.539229  acc: 88.5714%(62/70)\n",
            "Starting epoch 24, learning rate is 0.000886\n",
            "  Evaluation on train - loss: 0.583997  acc: 89.3491%(604/676)\n",
            "  Evaluation on dev - loss: 0.629907  acc: 85.7143%(60/70)\n",
            "Starting epoch 25, learning rate is 0.000798\n",
            "  Evaluation on train - loss: 0.464906  acc: 90.8284%(614/676)\n",
            "  Evaluation on dev - loss: 0.499318  acc: 90.0000%(63/70)\n",
            "Starting epoch 26, learning rate is 0.000718\n",
            "  Evaluation on train - loss: 0.480368  acc: 91.5680%(619/676)\n",
            "  Evaluation on dev - loss: 0.542370  acc: 92.8571%(65/70)\n",
            "Starting epoch 27, learning rate is 0.000646\n",
            "  Evaluation on train - loss: 0.421444  acc: 90.6805%(613/676)\n",
            "  Evaluation on dev - loss: 0.483063  acc: 92.8571%(65/70)\n",
            "Starting epoch 28, learning rate is 0.000581\n",
            "  Evaluation on train - loss: 0.377423  acc: 88.6095%(599/676)\n",
            "  Evaluation on dev - loss: 0.429541  acc: 90.0000%(63/70)\n",
            "Starting epoch 29, learning rate is 0.000523\n",
            "  Evaluation on train - loss: 0.463519  acc: 92.4556%(625/676)\n",
            "  Evaluation on dev - loss: 0.533971  acc: 91.4286%(64/70)\n",
            "Starting epoch 30, learning rate is 0.000471\n",
            "  Evaluation on train - loss: 0.410788  acc: 90.9763%(615/676)\n",
            "  Evaluation on dev - loss: 0.453981  acc: 90.0000%(63/70)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPDux4X2sY8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFh1olVKtpF2",
        "colab_type": "text"
      },
      "source": [
        "### Dynamic learning rate scheduling\n",
        "\n",
        "Another option is to use dynamic rate scheduling: decrease the learning rate every time model performance on dev data stops improving (i.e., when a \"plateau\" is reached. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j02UdSHZvhym",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_with_label_smoothing_adaptive_lr_schedule(model, num_epochs, train_iter, test_iter, device,  label_smoothing=0.1, lr_decay=0.5, log_interval=10):\n",
        "\n",
        "  assert (label_smoothing >= 0.0 and label_smoothing <= 1.0)\n",
        "  \n",
        "  # Each non-target class gets a target probability of label_smoothing / 2.0\n",
        "  non_target_prob = label_smoothing / 2.0\n",
        "  \n",
        "  # Usually, SGD is used instead of Adam, when learning rate scheduling is used\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "  \n",
        "  # every 2 epochs, we will multiply the learning rate by 0.5\n",
        "  lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=lr_decay, patience=1, verbose=True)\n",
        "\n",
        "  steps = 0\n",
        "  best_acc = 0\n",
        "  last_step = 0\n",
        "  \n",
        "  criterion = nn.KLDivLoss(reduction='sum')\n",
        "  \n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(\"Starting epoch %d\" % epoch)\n",
        "    for batch in train_iter:\n",
        "      # set training mode\n",
        "      model.train()\n",
        "      fbank, target = batch\n",
        "      fbank, target = fbank.to(device), target.to(device)\n",
        "      batch_size = target.shape[0]\n",
        "      \n",
        "      # Create a one-hot encoded target matrix of size (batch_size, 3)\n",
        "      # where the values corresponding to correct labelsa re 1.0-label_smoothing\n",
        "      # and everywhere else there is non_target_prob (equal to 0.05, if label_smoothing is 0.1)\n",
        "      target_one_hot = torch.ones(batch_size, 3).to(device)      \n",
        "      target_one_hot *= non_target_prob\n",
        "      target_one_hot[torch.arange(batch_size, dtype=torch.int64).to(device), target] = 1.0 - label_smoothing\n",
        "      \n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logit = model(fbank)\n",
        "      log_probabilities = F.log_softmax(logit)\n",
        "      loss = criterion(log_probabilities, target_one_hot).mean()\n",
        "      \n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "    train_acc, train_loss = evaluate(\"train\", train_iter, model)                \n",
        "    dev_acc, dev_loss = evaluate(\"dev\", dev_iter, model)       \n",
        "    lr_scheduler.step(dev_loss)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoeZySXIwKXc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne-oC54QwNaq",
        "colab_type": "code",
        "outputId": "bfa0539a-9af7-4e25-af86-6389318712ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1889
        }
      },
      "source": [
        "model_resnet = AccentResNet(3, train_dataset[0][0].shape[1]).to(device)\n",
        "train_with_label_smoothing_adaptive_lr_schedule(model_resnet, 30, train_iter, dev_iter, device=device, label_smoothing=0.2, lr_decay=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting epoch 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "  Evaluation on train - loss: 0.874490  acc: 77.2189%(522/676)\n",
            "  Evaluation on dev - loss: 0.838321  acc: 81.4286%(57/70)\n",
            "Starting epoch 2\n",
            "  Evaluation on train - loss: 0.681129  acc: 77.3669%(523/676)\n",
            "  Evaluation on dev - loss: 0.632447  acc: 81.4286%(57/70)\n",
            "Starting epoch 3\n",
            "  Evaluation on train - loss: 0.600213  acc: 77.5148%(524/676)\n",
            "  Evaluation on dev - loss: 0.472569  acc: 80.0000%(56/70)\n",
            "Starting epoch 4\n",
            "  Evaluation on train - loss: 0.555386  acc: 77.5148%(524/676)\n",
            "  Evaluation on dev - loss: 0.468457  acc: 81.4286%(57/70)\n",
            "Starting epoch 5\n",
            "  Evaluation on train - loss: 0.555806  acc: 77.3669%(523/676)\n",
            "  Evaluation on dev - loss: 0.472371  acc: 80.0000%(56/70)\n",
            "Starting epoch 6\n",
            "  Evaluation on train - loss: 0.640534  acc: 77.3669%(523/676)\n",
            "  Evaluation on dev - loss: 0.592279  acc: 84.2857%(59/70)\n",
            "Epoch     5: reducing learning rate of group 0 to 5.0000e-03.\n",
            "Starting epoch 7\n",
            "  Evaluation on train - loss: 0.565086  acc: 80.0296%(541/676)\n",
            "  Evaluation on dev - loss: 0.500273  acc: 85.7143%(60/70)\n",
            "Starting epoch 8\n",
            "  Evaluation on train - loss: 0.575167  acc: 81.8047%(553/676)\n",
            "  Evaluation on dev - loss: 0.550493  acc: 85.7143%(60/70)\n",
            "Epoch     7: reducing learning rate of group 0 to 2.5000e-03.\n",
            "Starting epoch 9\n",
            "  Evaluation on train - loss: 0.547655  acc: 85.0592%(575/676)\n",
            "  Evaluation on dev - loss: 0.531637  acc: 88.5714%(62/70)\n",
            "Starting epoch 10\n",
            "  Evaluation on train - loss: 0.523516  acc: 83.2840%(563/676)\n",
            "  Evaluation on dev - loss: 0.508013  acc: 85.7143%(60/70)\n",
            "Epoch     9: reducing learning rate of group 0 to 1.2500e-03.\n",
            "Starting epoch 11\n",
            "  Evaluation on train - loss: 0.512056  acc: 85.5030%(578/676)\n",
            "  Evaluation on dev - loss: 0.484578  acc: 91.4286%(64/70)\n",
            "Starting epoch 12\n",
            "  Evaluation on train - loss: 0.489360  acc: 83.8757%(567/676)\n",
            "  Evaluation on dev - loss: 0.500472  acc: 87.1429%(61/70)\n",
            "Epoch    11: reducing learning rate of group 0 to 6.2500e-04.\n",
            "Starting epoch 13\n",
            "  Evaluation on train - loss: 0.526208  acc: 86.9822%(588/676)\n",
            "  Evaluation on dev - loss: 0.520355  acc: 87.1429%(61/70)\n",
            "Starting epoch 14\n",
            "  Evaluation on train - loss: 0.500299  acc: 86.9822%(588/676)\n",
            "  Evaluation on dev - loss: 0.490828  acc: 90.0000%(63/70)\n",
            "Epoch    13: reducing learning rate of group 0 to 3.1250e-04.\n",
            "Starting epoch 15\n",
            "  Evaluation on train - loss: 0.482471  acc: 86.3905%(584/676)\n",
            "  Evaluation on dev - loss: 0.478742  acc: 90.0000%(63/70)\n",
            "Starting epoch 16\n",
            "  Evaluation on train - loss: 0.476184  acc: 86.0947%(582/676)\n",
            "  Evaluation on dev - loss: 0.477102  acc: 87.1429%(61/70)\n",
            "Epoch    15: reducing learning rate of group 0 to 1.5625e-04.\n",
            "Starting epoch 17\n",
            "  Evaluation on train - loss: 0.483401  acc: 87.2781%(590/676)\n",
            "  Evaluation on dev - loss: 0.487612  acc: 87.1429%(61/70)\n",
            "Starting epoch 18\n",
            "  Evaluation on train - loss: 0.488843  acc: 87.2781%(590/676)\n",
            "  Evaluation on dev - loss: 0.491084  acc: 88.5714%(62/70)\n",
            "Epoch    17: reducing learning rate of group 0 to 7.8125e-05.\n",
            "Starting epoch 19\n",
            "  Evaluation on train - loss: 0.498435  acc: 87.5740%(592/676)\n",
            "  Evaluation on dev - loss: 0.503504  acc: 88.5714%(62/70)\n",
            "Starting epoch 20\n",
            "  Evaluation on train - loss: 0.482151  acc: 87.2781%(590/676)\n",
            "  Evaluation on dev - loss: 0.490897  acc: 87.1429%(61/70)\n",
            "Epoch    19: reducing learning rate of group 0 to 3.9063e-05.\n",
            "Starting epoch 21\n",
            "  Evaluation on train - loss: 0.494295  acc: 87.5740%(592/676)\n",
            "  Evaluation on dev - loss: 0.502774  acc: 90.0000%(63/70)\n",
            "Starting epoch 22\n",
            "  Evaluation on train - loss: 0.491223  acc: 87.7219%(593/676)\n",
            "  Evaluation on dev - loss: 0.497776  acc: 90.0000%(63/70)\n",
            "Epoch    21: reducing learning rate of group 0 to 1.9531e-05.\n",
            "Starting epoch 23\n",
            "  Evaluation on train - loss: 0.482889  acc: 87.5740%(592/676)\n",
            "  Evaluation on dev - loss: 0.489592  acc: 88.5714%(62/70)\n",
            "Starting epoch 24\n",
            "  Evaluation on train - loss: 0.492765  acc: 87.7219%(593/676)\n",
            "  Evaluation on dev - loss: 0.504512  acc: 90.0000%(63/70)\n",
            "Epoch    23: reducing learning rate of group 0 to 9.7656e-06.\n",
            "Starting epoch 25\n",
            "  Evaluation on train - loss: 0.482595  acc: 87.5740%(592/676)\n",
            "  Evaluation on dev - loss: 0.489074  acc: 90.0000%(63/70)\n",
            "Starting epoch 26\n",
            "  Evaluation on train - loss: 0.487058  acc: 87.5740%(592/676)\n",
            "  Evaluation on dev - loss: 0.493914  acc: 90.0000%(63/70)\n",
            "Epoch    25: reducing learning rate of group 0 to 4.8828e-06.\n",
            "Starting epoch 27\n",
            "  Evaluation on train - loss: 0.472822  acc: 87.1302%(589/676)\n",
            "  Evaluation on dev - loss: 0.482744  acc: 85.7143%(60/70)\n",
            "Starting epoch 28\n",
            "  Evaluation on train - loss: 0.487242  acc: 87.5740%(592/676)\n",
            "  Evaluation on dev - loss: 0.497652  acc: 88.5714%(62/70)\n",
            "Epoch    27: reducing learning rate of group 0 to 2.4414e-06.\n",
            "Starting epoch 29\n",
            "  Evaluation on train - loss: 0.474241  acc: 87.2781%(590/676)\n",
            "  Evaluation on dev - loss: 0.481301  acc: 87.1429%(61/70)\n",
            "Starting epoch 30\n",
            "  Evaluation on train - loss: 0.485397  acc: 87.2781%(590/676)\n",
            "  Evaluation on dev - loss: 0.499019  acc: 90.0000%(63/70)\n",
            "Epoch    29: reducing learning rate of group 0 to 1.2207e-06.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmojFCsiwTVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}