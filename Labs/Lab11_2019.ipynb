{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab11_2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akaver/NLP2019/blob/master/Lab11_2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7F4dTOoSOC",
        "colab_type": "text"
      },
      "source": [
        "Partly based on https://github.com/Kyubyong/bert_ner"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uXs7D3wZvD_",
        "colab_type": "code",
        "outputId": "8f6cc7d6-2bf4-4fc3-8247-153ceb663ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "!pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/3c/d5fa084dd3a82ffc645aba78c417e6072ff48552e3301b1fa3bd711e03d4/pytorch_pretrained_bert-0.6.1-py3-none-any.whl (114kB)\n",
            "\u001b[K    100% |████████████████████████████████| 122kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.14.6)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2018.1.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (2.18.4)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.0.1.post2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert) (1.9.128)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (1.22)\n",
            "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert) (2.6)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.13.0,>=1.12.128 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (1.12.128)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert) (0.2.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3->pytorch-pretrained-bert) (2.5.3)\n",
            "Requirement already satisfied: docutils>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.13.0,>=1.12.128->boto3->pytorch-pretrained-bert) (0.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.13.0,>=1.12.128->boto3->pytorch-pretrained-bert) (1.11.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPHK2MqIab1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj3THDF-bMn6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q https://raw.githubusercontent.com/synalp/NER/master/corpus/CoNLL-2003/eng.testa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmfEgA_BbR86",
        "colab_type": "code",
        "outputId": "3223b75a-948f-40bd-aeb6-3da9cd47ce69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 213450/213450 [00:00<00:00, 382620.99B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFIirhrRbr9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VOCAB = ('<PAD>', 'O', 'LOC', 'PER', 'ORG', 'MISC')\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(VOCAB)}\n",
        "idx2tag = {idx: tag for idx, tag in enumerate(VOCAB)}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmXVKRuycsSO",
        "colab_type": "code",
        "outputId": "49841e1b-c11a-42d1-eb79-afad924698c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import torch \n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device('cuda')\n",
        "\n",
        "print(device)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzeGZxwAcUNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "An entry or sent looks like ...\n",
        "SOCCER NN B-NP O\n",
        "- : O O\n",
        "JAPAN NNP B-NP B-LOC\n",
        "GET VB B-VP O\n",
        "LUCKY NNP B-NP O\n",
        "WIN NNP I-NP O\n",
        ", , O O\n",
        "CHINA NNP B-NP B-PER\n",
        "IN IN B-PP O\n",
        "SURPRISE DT B-NP O\n",
        "DEFEAT NN I-NP O\n",
        ". . O O\n",
        "Each mini-batch returns the followings:\n",
        "words: list of input sents. [\"The 26-year-old ...\", ...]\n",
        "x: encoded input sents. [N, T]. int64.\n",
        "is_heads: list of head markers. [[1, 1, 0, ...], [...]]\n",
        "tags: list of tags.['O O B-MISC ...', '...']\n",
        "y: encoded tags. [N, T]. int64\n",
        "seqlens: list of seqlens. [45, 49, 10, 50, ...]\n",
        "'''\n",
        "class NerDataset(Dataset):\n",
        "    def __init__(self, fpath):\n",
        "        \"\"\"\n",
        "        fpath: [train|valid|test].txt\n",
        "        \"\"\"\n",
        "        entries = open(fpath, 'r').read().strip().split(\"\\n\\n\")\n",
        "        sents, tags_li = [], [] # list of lists\n",
        "        for entry in entries:\n",
        "            words = [line.split()[0] for line in entry.splitlines()]\n",
        "            tags = ([line.split()[-1] for line in entry.splitlines()])\n",
        "            tags = [l.lstrip(\"B-\").lstrip(\"I-\") for l in tags]\n",
        "            sents.append([\"[CLS]\"] + words + [\"[SEP]\"])\n",
        "            tags_li.append([\"<PAD>\"] + tags + [\"<PAD>\"])\n",
        "        self.sents, self.tags_li = sents, tags_li\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        words, tags = self.sents[idx], self.tags_li[idx] # words, tags: string list\n",
        "\n",
        "        # We give credits only to the first piece.\n",
        "        x, y = [], [] # list of ids\n",
        "        is_heads = [] # list. 1: the token is the first piece of a word\n",
        "        for w, t in zip(words, tags):\n",
        "            tokens = tokenizer.tokenize(w) if w not in (\"[CLS]\", \"[SEP]\") else [w]\n",
        "            xx = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "            is_head = [1] + [0]*(len(tokens) - 1)\n",
        "\n",
        "            t = [t] + [\"<PAD>\"] * (len(tokens) - 1)  # <PAD>: no decision\n",
        "            yy = [tag2idx[each] for each in t]  # (T,)\n",
        "\n",
        "            x.extend(xx)\n",
        "            is_heads.extend(is_head)\n",
        "            y.extend(yy)\n",
        "\n",
        "        assert len(x)==len(y)==len(is_heads), f\"len(x)={len(x)}, len(y)={len(y)}, len(is_heads)={len(is_heads)}\"\n",
        "\n",
        "        # seqlen\n",
        "        seqlen = len(y)\n",
        "\n",
        "        # to string\n",
        "        words = \" \".join(words)\n",
        "        tags = \" \".join(tags)\n",
        "        return words, x, is_heads, tags, y, seqlen\n",
        "\n",
        "\n",
        "def pad(batch):\n",
        "    '''Pads to the longest sample'''\n",
        "    f = lambda x: [sample[x] for sample in batch]\n",
        "    words = f(0)\n",
        "    is_heads = f(2)\n",
        "    tags = f(3)\n",
        "    seqlens = f(-1)\n",
        "    maxlen = np.array(seqlens).max()\n",
        "\n",
        "    f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: <pad>\n",
        "    x = f(1, maxlen)\n",
        "    y = f(-2, maxlen)\n",
        "\n",
        "\n",
        "    f = torch.LongTensor\n",
        "\n",
        "    return words, f(x), is_heads, tags, f(y), seqlens\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNFE_sXscl2W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pytorch_pretrained_bert import BertModel\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, top_rnns=False, vocab_size=None, device='cpu', finetuning=False):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "        self.top_rnns=top_rnns\n",
        "        if top_rnns:\n",
        "            self.rnn = nn.LSTM(bidirectional=True, num_layers=2, input_size=768, hidden_size=768//2, batch_first=True)\n",
        "        self.fc = nn.Linear(768, vocab_size)\n",
        "\n",
        "        self.device = device\n",
        "        self.finetuning = finetuning\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: (N, T). int64\n",
        "        y: (N, T). int64\n",
        "        Returns\n",
        "        enc: (N, T, VOCAB)\n",
        "        '''\n",
        "        x = x.to(self.device)\n",
        "\n",
        "\n",
        "        if self.training and self.finetuning:\n",
        "            # print(\"->bert.train()\")\n",
        "            self.bert.train()\n",
        "            encoded_layers, _ = self.bert(x)\n",
        "            enc = encoded_layers[-1]\n",
        "        else:\n",
        "            self.bert.eval()\n",
        "            with torch.no_grad():\n",
        "                encoded_layers, _ = self.bert(x)\n",
        "                enc = encoded_layers[-1]\n",
        "\n",
        "        if self.top_rnns:\n",
        "            enc, _ = self.rnn(enc)\n",
        "        logits = self.fc(enc)\n",
        "        \n",
        "        return logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PussNh4cdEnn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = NerDataset(\"eng.train\")\n",
        "dev_dataset = NerDataset(\"eng.testa\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jug9ZZPxeCsU",
        "colab_type": "code",
        "outputId": "aa290457-7b4f-4e69-ba43-1cab3921aa9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        }
      },
      "source": [
        "train_dataset[1]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[CLS] EU rejects German call to boycott British lamb . [SEP]',\n",
              " [101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1],\n",
              " '<PAD> ORG O MISC O O O MISC O O <PAD>',\n",
              " [0, 4, 1, 5, 1, 1, 1, 5, 1, 0, 1, 0],\n",
              " 12)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEphkyZRq4V4",
        "colab_type": "code",
        "outputId": "a8f19536-f1bf-40cf-81e1-c316a230fcac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "# for calculating metrics\n",
        "!pip install sklearn_crfsuite\n",
        "import sklearn_crfsuite\n",
        "import sklearn_crfsuite.metrics"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn_crfsuite\n",
            "  Downloading https://files.pythonhosted.org/packages/25/74/5b7befa513482e6dee1f3dd68171a6c9dfc14c0eaa00f885ffeba54fe9b0/sklearn_crfsuite-0.3.6-py2.py3-none-any.whl\n",
            "Collecting python-crfsuite>=0.8.3 (from sklearn_crfsuite)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/86/cfcd71edca9d25d3d331209a20f6314b6f3f134c29478f90559cee9ce091/python_crfsuite-0.9.6-cp36-cp36m-manylinux1_x86_64.whl (754kB)\n",
            "\r\u001b[K    1% |▍                               | 10kB 18.2MB/s eta 0:00:01\r\u001b[K    2% |▉                               | 20kB 1.7MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 30kB 2.5MB/s eta 0:00:01\r\u001b[K    5% |█▊                              | 40kB 3.3MB/s eta 0:00:01\r\u001b[K    6% |██▏                             | 51kB 4.1MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 61kB 4.8MB/s eta 0:00:01\r\u001b[K    9% |███                             | 71kB 5.5MB/s eta 0:00:01\r\u001b[K    10% |███▌                            | 81kB 3.4MB/s eta 0:00:01\r\u001b[K    12% |████                            | 92kB 3.7MB/s eta 0:00:01\r\u001b[K    13% |████▍                           | 102kB 4.1MB/s eta 0:00:01\r\u001b[K    14% |████▉                           | 112kB 4.2MB/s eta 0:00:01\r\u001b[K    16% |█████▏                          | 122kB 7.6MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 133kB 7.6MB/s eta 0:00:01\r\u001b[K    19% |██████                          | 143kB 7.6MB/s eta 0:00:01\r\u001b[K    20% |██████▌                         | 153kB 7.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 163kB 7.7MB/s eta 0:00:01\r\u001b[K    23% |███████▍                        | 174kB 7.6MB/s eta 0:00:01\r\u001b[K    24% |███████▉                        | 184kB 8.4MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 194kB 8.4MB/s eta 0:00:01\r\u001b[K    27% |████████▊                       | 204kB 8.1MB/s eta 0:00:01\r\u001b[K    28% |█████████▏                      | 215kB 8.0MB/s eta 0:00:01\r\u001b[K    29% |█████████▋                      | 225kB 8.1MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 235kB 8.1MB/s eta 0:00:01\r\u001b[K    32% |██████████▍                     | 245kB 8.1MB/s eta 0:00:01\r\u001b[K    33% |██████████▉                     | 256kB 8.1MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 266kB 8.0MB/s eta 0:00:01\r\u001b[K    36% |███████████▊                    | 276kB 8.1MB/s eta 0:00:01\r\u001b[K    38% |████████████▏                   | 286kB 38.0MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 296kB 8.1MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 307kB 8.4MB/s eta 0:00:01\r\u001b[K    42% |█████████████▌                  | 317kB 8.5MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 327kB 8.5MB/s eta 0:00:01\r\u001b[K    44% |██████████████▍                 | 337kB 8.5MB/s eta 0:00:01\r\u001b[K    46% |██████████████▊                 | 348kB 8.3MB/s eta 0:00:01\r\u001b[K    47% |███████████████▏                | 358kB 8.3MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 368kB 8.5MB/s eta 0:00:01\r\u001b[K    50% |████████████████                | 378kB 8.4MB/s eta 0:00:01\r\u001b[K    51% |████████████████▌               | 389kB 8.4MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 399kB 47.0MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▍              | 409kB 46.8MB/s eta 0:00:01\r\u001b[K    55% |█████████████████▉              | 419kB 48.4MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 430kB 47.8MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▊             | 440kB 47.5MB/s eta 0:00:01\r\u001b[K    59% |███████████████████▏            | 450kB 48.0MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▌            | 460kB 48.2MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 471kB 48.6MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▍           | 481kB 49.2MB/s eta 0:00:01\r\u001b[K    65% |████████████████████▉           | 491kB 49.4MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 501kB 49.4MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▊          | 512kB 43.9MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▏         | 522kB 43.7MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 532kB 44.2MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 542kB 45.3MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▌        | 552kB 48.8MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 563kB 47.2MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▎       | 573kB 46.8MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▊       | 583kB 47.0MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 593kB 46.9MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▋      | 604kB 46.8MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████      | 614kB 54.1MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 624kB 14.4MB/s eta 0:00:01\r\u001b[K    84% |███████████████████████████     | 634kB 14.3MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▍    | 645kB 14.2MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▉    | 655kB 14.2MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▎   | 665kB 13.4MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▊   | 675kB 13.4MB/s eta 0:00:01\r\u001b[K    90% |█████████████████████████████▏  | 686kB 13.4MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 696kB 13.4MB/s eta 0:00:01\r\u001b[K    93% |██████████████████████████████  | 706kB 13.3MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▍ | 716kB 13.3MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 727kB 40.3MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▎| 737kB 41.5MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▊| 747kB 42.6MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 757kB 20.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.11.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.28.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.3)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite\n",
            "Successfully installed python-crfsuite-0.9.6 sklearn-crfsuite-0.3.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGla9WwheEw1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train(model, num_epochs, train_iter, dev_iter):\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "  best_acc = 0\n",
        "  last_step = 0\n",
        "  for epoch in range(1, num_epochs+1):\n",
        "    print(\"Epoch %d\" % epoch)\n",
        "    \n",
        "    for i, batch in enumerate(train_iter):\n",
        "        model.train()\n",
        "        words, x, is_heads, tags, y, seqlens = batch\n",
        "        _y = y # for monitoring\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(x) # logits: (N, T, VOCAB), y: (N, T)\n",
        "\n",
        "        logits = logits.view(-1, logits.shape[-1]) # (N*T, VOCAB)\n",
        "        y = y.to(device)\n",
        "        y = y.view(-1)  # (N*T,)\n",
        "\n",
        "        loss = criterion(logits, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if i == 0:\n",
        "            print(\"=====sanity check======\")\n",
        "            print(\"words:\", words[0])\n",
        "            print(\"x:\", x.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tokens:\", tokenizer.convert_ids_to_tokens(x.cpu().numpy()[0])[:seqlens[0]])\n",
        "            print(\"is_heads:\", is_heads[0])\n",
        "            print(\"y:\", _y.cpu().numpy()[0][:seqlens[0]])\n",
        "            print(\"tags:\", tags[0])\n",
        "            print(\"seqlen:\", seqlens[0])\n",
        "            print(\"=======================\")\n",
        "\n",
        "\n",
        "        if i%10 == 0: # monitoring\n",
        "            print(f\"step: {i}, loss: {loss.item()}\")\n",
        "\n",
        "        if i%100 == 0: # let's evaluate more frequently than evry epoch\n",
        "            evaluate(\"dev\", dev_iter, model)\n",
        "            \n",
        "\n",
        "\n",
        "\n",
        "def evaluate(dataset_name, data_iter, model, full_report=False):\n",
        "  \n",
        "  model.eval()\n",
        "  y_pred_seq = []\n",
        "  y_seq = []\n",
        "  with torch.no_grad():\n",
        "    for batch in data_iter:\n",
        "      words, x, is_heads, tags, y, seqlens = batch\n",
        "\n",
        "      logits = model(x)  # y_hat: (N, T)\n",
        "      y_pred = logits.argmax(-1)\n",
        "      \n",
        "      \n",
        "      for i in range(len(y)):\n",
        "        head_i = is_heads[i]\n",
        "        y_i = y[i].cpu().numpy()\n",
        "        y_pred_i = y_pred[i].cpu().numpy()\n",
        "            \n",
        "        y_pred_i = [VOCAB[y_pred_j] for head, y_pred_j in zip(head_i, y_pred_i) if head == 1][1:-1]\n",
        "        y_i = [VOCAB[y_j] for head, y_j in zip(head_i, y_i) if head == 1][1:-1]\n",
        "\n",
        "        y_pred_seq.append(y_pred_i)\n",
        "        y_seq.append(y_i)\n",
        "  \n",
        "  accuracy = sklearn_crfsuite.metrics.flat_accuracy_score(y_seq, y_pred_seq)\n",
        "  \n",
        "  print('  Evaluation on {} -  acc: {:.4f}%'.format(dataset_name, accuracy))\n",
        "  if full_report:\n",
        "    print(sklearn_crfsuite.metrics.flat_classification_report(y_seq, y_pred_seq, labels=[\"LOC\", \"MISC\", \"ORG\", \"PER\"]))\n",
        "       \n",
        "\n",
        "   \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmAXd6mtgdyY",
        "colab_type": "code",
        "outputId": "09af966c-1af9-4579-d6d8-cd2ac62b7bc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model = Net(False, len(VOCAB), device, True).to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 404400730/404400730 [00:34<00:00, 11830956.07B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxmmsVPPglxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_iter = DataLoader(dataset=train_dataset,\n",
        "                                 batch_size=batch_size,\n",
        "                                 shuffle=True,\n",
        "                                 num_workers=4,\n",
        "                                 collate_fn=pad)\n",
        "dev_iter = DataLoader(dataset=dev_dataset,\n",
        "                                 batch_size=batch_size,\n",
        "                                 shuffle=False,\n",
        "                                 num_workers=4,\n",
        "                                 collate_fn=pad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DSZG1gh_hhdc",
        "colab_type": "code",
        "outputId": "b601bc55-437c-4a1b-b055-a6aa6bdac3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2026
        }
      },
      "source": [
        "train(model, 1, train_iter, dev_iter)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "=====sanity check======\n",
            "words: [CLS] CRICKET - GOOCH TO PLAY ANOTHER SEASON FOR ESSEX . [SEP]\n",
            "x: [  101 15531  9741 22441  1942   118 27157  9244  3048 16972   153 10783\n",
            "  3663 23096 14697  3048  9637 12342 10719 11414   143  9565   142 12480\n",
            " 24654   119   102]\n",
            "tokens: ['[CLS]', 'CR', '##IC', '##KE', '##T', '-', 'GO', '##OC', '##H', 'TO', 'P', '##LA', '##Y', 'AN', '##OT', '##H', '##ER', 'SE', '##AS', '##ON', 'F', '##OR', 'E', '##SS', '##EX', '.', '[SEP]']\n",
            "is_heads: [1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1]\n",
            "y: [0 1 0 0 0 1 3 0 0 1 1 0 0 1 0 0 0 1 0 0 1 0 2 0 0 1 0]\n",
            "tags: <PAD> O O PER O O O O O LOC O <PAD>\n",
            "seqlen: 27\n",
            "=======================\n",
            "step: 0, loss: 1.72129225730896\n",
            "  Evaluation on dev -  acc: 0.8312%\n",
            "step: 10, loss: 0.4828910827636719\n",
            "step: 20, loss: 0.27751442790031433\n",
            "step: 30, loss: 0.132721409201622\n",
            "step: 40, loss: 0.09761400520801544\n",
            "step: 50, loss: 0.17915427684783936\n",
            "step: 60, loss: 0.18298524618148804\n",
            "step: 70, loss: 0.13008172810077667\n",
            "step: 80, loss: 0.05610474944114685\n",
            "step: 90, loss: 0.08222338557243347\n",
            "step: 100, loss: 0.054114554077386856\n",
            "  Evaluation on dev -  acc: 0.9772%\n",
            "step: 110, loss: 0.14841040968894958\n",
            "step: 120, loss: 0.08178847283124924\n",
            "step: 130, loss: 0.09883850812911987\n",
            "step: 140, loss: 0.07631660252809525\n",
            "step: 150, loss: 0.059778183698654175\n",
            "step: 160, loss: 0.029981523752212524\n",
            "step: 170, loss: 0.07270228117704391\n",
            "step: 180, loss: 0.08560604602098465\n",
            "step: 190, loss: 0.09732101857662201\n",
            "step: 200, loss: 0.10500702261924744\n",
            "  Evaluation on dev -  acc: 0.9833%\n",
            "step: 210, loss: 0.08659526705741882\n",
            "step: 220, loss: 0.06244508549571037\n",
            "step: 230, loss: 0.03064742125570774\n",
            "step: 240, loss: 0.09167353063821793\n",
            "step: 250, loss: 0.13631278276443481\n",
            "step: 260, loss: 0.0925740972161293\n",
            "step: 270, loss: 0.03986118733882904\n",
            "step: 280, loss: 0.05725754797458649\n",
            "step: 290, loss: 0.1588253378868103\n",
            "step: 300, loss: 0.031236328184604645\n",
            "  Evaluation on dev -  acc: 0.9852%\n",
            "step: 310, loss: 0.051997460424900055\n",
            "step: 320, loss: 0.12914061546325684\n",
            "step: 330, loss: 0.03864322975277901\n",
            "step: 340, loss: 0.09592825919389725\n",
            "step: 350, loss: 0.03316628560423851\n",
            "step: 360, loss: 0.044975075870752335\n",
            "step: 370, loss: 0.07067961245775223\n",
            "step: 380, loss: 0.04914141818881035\n",
            "step: 390, loss: 0.10206964612007141\n",
            "step: 400, loss: 0.056376345455646515\n",
            "  Evaluation on dev -  acc: 0.9826%\n",
            "step: 410, loss: 0.054925914853811264\n",
            "step: 420, loss: 0.01962660253047943\n",
            "step: 430, loss: 0.15647651255130768\n",
            "step: 440, loss: 0.014396685175597668\n",
            "step: 450, loss: 0.015697695314884186\n",
            "step: 460, loss: 0.12793698906898499\n",
            "step: 470, loss: 0.2235373705625534\n",
            "step: 480, loss: 0.22093813121318817\n",
            "step: 490, loss: 0.11245547980070114\n",
            "step: 500, loss: 0.17628435790538788\n",
            "  Evaluation on dev -  acc: 0.9831%\n",
            "step: 510, loss: 0.03950946033000946\n",
            "step: 520, loss: 0.031075408682227135\n",
            "step: 530, loss: 0.006124676205217838\n",
            "step: 540, loss: 0.06230213865637779\n",
            "step: 550, loss: 0.06743279099464417\n",
            "step: 560, loss: 0.03417086973786354\n",
            "step: 570, loss: 0.07167670875787735\n",
            "step: 580, loss: 0.014637463726103306\n",
            "step: 590, loss: 0.007606813684105873\n",
            "step: 600, loss: 0.0389721542596817\n",
            "  Evaluation on dev -  acc: 0.9859%\n",
            "step: 610, loss: 0.04228334128856659\n",
            "step: 620, loss: 0.04846424609422684\n",
            "step: 630, loss: 0.01929917372763157\n",
            "step: 640, loss: 0.10231783986091614\n",
            "step: 650, loss: 0.031070245429873466\n",
            "step: 660, loss: 0.012764958664774895\n",
            "step: 670, loss: 0.026508236303925514\n",
            "step: 680, loss: 0.029334781691432\n",
            "step: 690, loss: 0.030691644176840782\n",
            "step: 700, loss: 0.05757085978984833\n",
            "  Evaluation on dev -  acc: 0.9852%\n",
            "step: 710, loss: 0.13545262813568115\n",
            "step: 720, loss: 0.08133676648139954\n",
            "step: 730, loss: 0.04363226145505905\n",
            "step: 740, loss: 0.049268897622823715\n",
            "step: 750, loss: 0.06646779179573059\n",
            "step: 760, loss: 0.03360796719789505\n",
            "step: 770, loss: 0.046891503036022186\n",
            "step: 780, loss: 0.03923836350440979\n",
            "step: 790, loss: 0.09371566027402878\n",
            "step: 800, loss: 0.18766801059246063\n",
            "  Evaluation on dev -  acc: 0.9864%\n",
            "step: 810, loss: 0.027112463489174843\n",
            "step: 820, loss: 0.04998226836323738\n",
            "step: 830, loss: 0.053685836493968964\n",
            "step: 840, loss: 0.057726725935935974\n",
            "step: 850, loss: 0.09425348788499832\n",
            "step: 860, loss: 0.031133225187659264\n",
            "step: 870, loss: 0.10388842225074768\n",
            "step: 880, loss: 0.0645998865365982\n",
            "step: 890, loss: 0.11647999286651611\n",
            "step: 900, loss: 0.05231634899973869\n",
            "  Evaluation on dev -  acc: 0.9877%\n",
            "step: 910, loss: 0.0822499692440033\n",
            "step: 920, loss: 0.02857440710067749\n",
            "step: 930, loss: 0.09193854033946991\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3QXyGoskElG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmAfq3fnd964",
        "colab_type": "code",
        "outputId": "48a19a87-20d7-49b7-833f-915c0af223d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "evaluate(\"dev\", dev_iter, model, full_report=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Evaluation on dev -  acc: 0.9885%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         LOC       0.97      0.96      0.96      2094\n",
            "        MISC       0.89      0.88      0.88      1268\n",
            "         ORG       0.94      0.92      0.93      2092\n",
            "         PER       0.98      0.98      0.98      3149\n",
            "\n",
            "   micro avg       0.95      0.94      0.95      8603\n",
            "   macro avg       0.94      0.93      0.94      8603\n",
            "weighted avg       0.95      0.94      0.95      8603\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZUILt-5J0PW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}